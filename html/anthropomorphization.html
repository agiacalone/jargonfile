<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<html xmlns="http://www.w3.org/1999/xhtml"><head><title>Anthropomorphization</title><link rel="stylesheet" href="jargon.css" type="text/css"/><meta name="generator" content="DocBook XSL Stylesheets V1.61.0"/><link rel="home" href="index.html" title="The Jargon File"/><link rel="up" href="construction.html" title="Chapter 4. Jargon Construction"/><link rel="previous" href="inarticulations.html" title="Spoken inarticulations"/><link rel="next" href="comparatives.html" title="Comparatives"/></head><body><div class="navheader"><table width="100%" summary="Navigation header"><tr><th colspan="3" align="center">Anthropomorphization</th></tr><tr><td width="20%" align="left"><a accesskey="p" href="inarticulations.html">Prev</a> </td><th width="60%" align="center">Chapter 4. Jargon Construction</th><td width="20%" align="right"> <a accesskey="n" href="comparatives.html">Next</a></td></tr></table><hr/></div><div class="sect1" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="anthropomorphization"/>Anthropomorphization</h2></div></div><div/></div><p>Semantically, one rich source of jargon constructions is the hackish
tendency to anthropomorphize hardware and software.  English purists and
academic computer scientists frequently look down on others for
anthropomorphizing hardware and software, considering this sort of behavior to
be characteristic of naive misunderstanding. But most hackers anthropomorphize
freely, frequently describing program behavior in terms of wants and
desires.</p><p>Thus it is common to hear hardware or software talked about as though it
has homunculi talking to each other inside it, with intentions and desires.
Thus, one hears “<span class="quote">The protocol handler got confused</span>”, or that
programs “<span class="quote">are trying</span>” to do things, or one may say of a routine
that “<span class="quote">its goal in life is to X</span>”.  Or: “<span class="quote">You can't run those
two cards on the same bus; they fight over interrupt 9.</span>”</p><p>One even hears explanations like “... and its poor little
brain couldn't understand X, and it died.” Sometimes modelling things
this way actually seems to make them easier to understand, perhaps because
it's instinctively natural to think of anything with a really complex
behavioral repertoire as ‘like a person’ rather than ‘like a
thing’.</p><p>At first glance, to anyone who understands how these programs actually
work, this seems like an absurdity.  As hackers are among the people who know
best how these phenomena work, it seems odd that they would use language that
seems to ascribe consciousness to them.  The mind-set behind this tendency
thus demands examination.</p><p>The key to understanding this kind of usage is that it isn't done in a
naive way; hackers don't personalize their stuff in the sense of feeling
empathy with it, nor do they mystically believe that the things they work on
every day are ‘alive’.  To the contrary: hackers who
anthropomorphize are expressing not a vitalistic view of program behavior but
a mechanistic view of human behavior.</p><p>Almost all hackers subscribe to the mechanistic, materialistic ontology
of science (this is in practice true even of most of the minority with
contrary religious theories).  In this view, people are biological machines
— consciousness is an interesting and valuable epiphenomenon, but mind is
implemented in machinery which is not fundamentally different in
information-processing capacity from computers.  </p><p>Hackers tend to take this a step further and argue that the difference
between a substrate of CHON atoms and water and a substrate of silicon and
metal is a relatively unimportant one; what matters, what makes a thing
‘alive’, is information and richness of pattern. This is animism
from the flip side; it implies that humans and computers and dolphins and
rocks are all machines exhibiting a continuum of modes of
‘consciousness’ according to their information-processing
capacity.</p><p>Because hackers accept that a human machine can have intentions, it is
therefore easy for them to ascribe consciousness and intention to other
complex patterned systems such as computers.  If consciousness is mechanical,
it is neither more or less absurd to say that “<span class="quote">The program wants to go
into an infinite loop</span>” than it is to say that “<span class="quote">I want to go eat
some chocolate</span>” — and even defensible to say that “<span class="quote">The
stone, once dropped, wants to move towards the center of the
earth</span>”.</p><p>This viewpoint has respectable company in academic philosophy.  Daniel
Dennett organizes explanations of behavior using three stances: the
“<span class="quote">physical stance</span>” (thing-to-be-explained as a physical object),
the “<span class="quote">design stance</span>” (thing-to-be-explained as an artifact), and
the “<span class="quote">intentional stance</span>” (thing-to-be-explained as an agent with
desires and intentions).  Which stances are appropriate is a matter not of
abstract truth but of utility.  Hackers typically view simple programs from
the design stance, but more complex ones are often modelled using the
intentional stance.</p><p>It has also been argued that the anthropomorphization of software and
hardware reflects a blurring of the boundary between the programmer and his
artifacts — the human qualities belong to the programmer and the code merely
expresses these qualities as his/her proxy.  On this view, a hacker saying a
piece of code ‘got confused’ is really saying that
<span class="emphasis"><em>he</em></span> (or she) was confused about exactly what he wanted the
computer to do, the code naturally incorporated this confusion, and the code
expressed the programmer's confusion when executed by crashing or otherwise
misbehaving.  </p><p>Note that by displacing from “<span class="quote">I got confused</span>” to “<span class="quote">It
got confused</span>”, the programmer is not avoiding responsibility, but
rather getting some analytical distance in order to be able to consider the
bug dispassionately.</p><p>It has also been suggested that anthropomorphizing complex systems is
actually an expression of humility, a way of acknowleging that simple rules we
do understand (or that we invented) can lead to emergent behavioral
complexities that we don't completely understand.</p><p>All three explanations accurately model hacker psychology, and should be
considered complementary rather than competing.</p></div><div class="navfooter"><hr/><table width="100%" summary="Navigation footer"><tr><td width="40%" align="left"><a accesskey="p" href="inarticulations.html">Prev</a> </td><td width="20%" align="center"><a accesskey="u" href="construction.html">Up</a></td><td width="40%" align="right"> <a accesskey="n" href="comparatives.html">Next</a></td></tr><tr><td width="40%" align="left" valign="top">Spoken inarticulations </td><td width="20%" align="center"><a accesskey="h" href="index.html">Home</a></td><td width="40%" align="right" valign="top"> Comparatives</td></tr></table></div></body></html>
